# -*- coding: utf-8 -*-
"""womakers.trabalho2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uTfQqEzOkbX70TZC4J7AnVvRgYlmE1AK

## Instalações necessárias
"""

!pip install dbfread
!pip install geopandas

import pandas as pd
from dbfread import DBF
from google.colab import drive
import geopandas as gpd

"""Conectando com o drive onde está a base de dados"""

drive.mount('/content/drive')

caminho_arquivo = '/content/drive/MyDrive/womakers.desmatamento/yearly_deforestation_biome.dbf'

from google.colab import drive
drive.mount('/content/drive')

"""# Aviso de desmatamento pego por satélites diariamente

Transformando em tabela
"""

tabela_dbf = DBF(caminho_arquivo)

df = pd.DataFrame(iter(tabela_dbf))

df.head()

"""Tirando colunas desnecessárias"""

colunas_desejadas = [
    'state',
    'sub_class',
    'image_date',
    'area_km'
]

df1 = df[colunas_desejadas]

"""Renomeando colunas para o português"""

df_aviso = df1.rename(columns={'state': 'estado', 'sub_class' : 'tipo_degradacao', 'image_date' : 'data_imagem'})
df_aviso.head()

"""Variáveis derivadas da data"""

df_aviso['data_imagem'] = pd.to_datetime(df_aviso['data_imagem'], errors='coerce')
df_aviso['ano'] = df_aviso['data_imagem'].dt.year
df_aviso['mes'] = df_aviso['data_imagem'].dt.month
df_aviso['dia'] = df_aviso['data_imagem'].dt.day
df_aviso['ano_mes'] = df_aviso['data_imagem'].dt.to_period('M').astype(str)

"""Área degradada por ano"""

df_aviso_ano = df_aviso.groupby('ano')['area_km'].sum().reset_index()
df_aviso_ano

"""Área degradada por ano/mês"""

df_aviso_ano_mes = df_aviso.groupby('ano_mes')['area_km'].sum().reset_index()
df_aviso_ano_mes

"""Área Total Degradada por Estado em km² e em %"""

df_area_estado = df_aviso.groupby('estado')['area_km'].sum().reset_index()
df_area_estado.rename(columns={'area_km': 'area_total_estado_km2'}, inplace=True)

total_geral = df_area_estado['area_total_estado_km2'].sum()
df_area_estado['%_estado'] = (df_area_estado['area_total_estado_km2'] / total_geral) * 100

df_area_estado

"""Limpeza de tipos de degradação inconsistentes"""

df_aviso = df_aviso[~df_aviso['tipo_degradacao'].str.match(r'^d\d{4}$')]

"""Área total degradada por tipos em km² e em %"""

df_area_tipo = df_aviso.groupby('tipo_degradacao')['area_km'].sum().reset_index()
df_area_tipo.rename(columns={'area_km': 'area_total_tipo_km2'}, inplace=True)

total = df_area_tipo['area_total_tipo_km2'].sum()
df_area_tipo['%_tipo'] = (df_area_tipo['area_total_tipo_km2'] / total) * 100

df_area_tipo

"""# Quanto da amazonia foi desmatado por ano"""

caminho_arquivo2 = '/content/drive/MyDrive/womakers.desmatamento/terrabrasilis_legal_amazon_14_11_2025_1763154652491.csv'

df2 = pd.read_csv(caminho_arquivo2, sep=';')

"""Nomes padronizados"""

df2.rename(columns={
    'year': 'ano',
    'area km²': 'area_km2',
    'uf': 'estado'
}, inplace=True)

"""Mudando a coluna de area km² para float"""

df2['area_km2'] = (
    df2['area_km2']
    .str.replace('.', '', regex=False)
    .str.replace(',', '.', regex=False)
    .astype(float)
)

"""Total desmatado por ano"""

df_total_ano = df2.groupby('ano')['area_km2'].sum().reset_index()
df_total_ano

"""Ranking de estados dentro de cada ano"""

df_ranking_estados = (
    df2
    .groupby(['ano', 'estado'])['area_km2']
    .sum()
    .reset_index()
    .sort_values(['ano', 'area_km2'], ascending=[True, False])
)
df_ranking_estados

"""Percentual por estado dentro de cada ano"""

df2_percentual = df2.copy()

total_por_ano = df2.groupby('ano')['area_km2'].sum()

df2_percentual['percentual'] = (
    df2_percentual['area_km2'] /
    df2_percentual['ano'].map(total_por_ano) * 100
)

df2_percentual.head()

"""Estatísticas por ano"""

df_estatisticas = df2.groupby('ano')['area_km2'].agg(
    media='mean',
    mediana='median',
    desvio_padrao='std'
)

df_estatisticas

"""Crescimento ano a ano (YOY)"""

df_total_ano['YOY'] = df_total_ano['area_km2'].pct_change() * 100
df_total_ano

"""Crescimento de longo prazo (CAGR)"""

ano_inicial = df2['ano'].min()
ano_final   = df2['ano'].max()

area_inicial = df2.loc[df2['ano'] == ano_inicial, 'area_km2'].values[0]
area_final   = df2.loc[df2['ano'] == ano_final, 'area_km2'].values[0]

n = ano_final - ano_inicial

CAGR = (area_final / area_inicial) ** (1/n) - 1

print(f"CAGR: {CAGR:.2%}")

"""# Queimadas por mês"""

caminho_arquivo3 = '/content/drive/MyDrive/womakers.desmatamento/historico_regiao_amazonia_legal.csv'

df3 = pd.read_csv(caminho_arquivo3)

"""Média Mensal"""

df3['media_mensal'] = df3.iloc[:, 1:13].mean(axis=1)

"""Mês de pico (considerado o mês ápice das queimadas em cada ano)"""

df3['mes_pico'] = df3.iloc[:, 1:13].idxmax(axis=1)

"""Variação percentual de um ano para o outro"""

df3['yoy_total'] = df3['Total'].pct_change() * 100

"""Média móvel de 3 anos"""

df3['rolling3_total'] = df3['Total'].rolling(3).mean()

df3.head()

"""###Validar a qualidade dos dados

Instalação  PANDERA
"""

pip install pandera

"""Definir um Schema

"""

import pandera as pa
from pandera import Column, DataFrameSchema, Check

schema_df_aviso = DataFrameSchema({
    "estado": Column(str, Check.str_length(min_value=2, max_value=2)),
    "tipo_degradacao": Column(str, Check.str_length(min_value=1)),
    "data_imagem": Column(pa.DateTime, nullable=True),  # aqui sim funciona
    "area_km": Column(float, Check.greater_than_or_equal_to(0.0)),
    "ano": Column(int, Check.in_range(1980, 2030)),
    "mes": Column(int, Check.in_range(1, 12)),
    "dia": Column(int, Check.in_range(1, 31)),
    "ano_mes": Column(str, Check.str_matches(r"^\d{4}-\d{2}$"))
})

"""Converter colunas para os tipos corretos"""

import pandas as pd

# Exemplo de conversão
df_aviso["data_imagem"] = pd.to_datetime(df_aviso["data_imagem"], errors="coerce")
df_aviso["ano"] = df_aviso["ano"].astype(int)
df_aviso["mes"] = df_aviso["mes"].astype(int)
df_aviso["dia"] = df_aviso["dia"].astype(int)
df_aviso["area_km"] = df_aviso["area_km"].astype(float)

validated_df = schema_df_aviso.validate(df_aviso)

try:
    validated_df = schema_df_aviso.validate(df_aviso, lazy=True)
    print(" Dados validados com sucesso! Nenhuma inconsistência encontrada.")
except pa.errors.SchemaErrors as err:
    print("Erros encontrados na validação:")
    print(err.failure_cases)  # mostra as linhas/colunas que falharam

"""### Camada Bronze

Definir o caminho
"""

output_path = '/content/drive/MyDrive/womakers.desmatamento/validated_deforestation_data.csv'

"""Salvando o DataFrame validado (df_aviso) como CSV


"""

df_aviso.to_csv(output_path, index=False)

print(f"\nBronze layer criada com sucesso! Dados validados salvos em: {output_path}")

df_bronze = pd.read_csv(output_path, parse_dates=['data_imagem'])

print("\nPrimeiras linhas do DataFrame da camada Bronze:")
df_bronze.head()

"""### Camada Silver"""

df_silver = pd.read_csv("/content/drive/MyDrive/womakers.desmatamento/validated_deforestation_data.csv")

"""Importação dos dados validados para o DataFrame Bronze"""

input_path = '/content/drive/MyDrive/womakers.desmatamento/validated_deforestation_data.csv'
df_silver = pd.read_csv(input_path, parse_dates=['data_imagem'])

"""Agrupamento dos dados em semestres"""

df_silver['semestre'] = df_bronze['data_imagem'].dt.month.apply(lambda x: 1 if 1 <= x <= 6 else 2)
df_silver.head()

"""Criação do diretório da camada Silver

"""

silver_output_dir = '/content/drive/MyDrive/womakers.desmatamento/camada_silver/'
import os
os.makedirs(silver_output_dir, exist_ok=True)
print(f"Diretório da camada silver criado ou já existente: {silver_output_dir}")

"""Exportação dos dados processados para a camada Silver


"""

silver_output_path = os.path.join(silver_output_dir, 'deforestation_silver_layer.csv')
df_silver.to_csv(silver_output_path, index=False)

print(f"Silver layer criada com sucesso! Dados processados salvos em: {silver_output_path}")

"""Dados da Camada Silver"""

print(df_silver["area_km"].describe())

print(df_silver.duplicated().sum())

print(df_silver["data_imagem"].min(), df_silver["data_imagem"].max())

df_silver.head()

"""# Explicação Detalhada

Nesta parte do projeto, realizamos uma série de passos para analisar dados de desmatamento e queimadas na Amazônia Legal. O processo foi dividido em:

1.  **Instalação e Importação de Bibliotecas**: Preparação do ambiente.
2.  **Carregamento e Pré-processamento do Primeiro Conjunto de Dados (Alertas de Desmatamento)**: Foco nos alertas de desmatamento diários.
3.  **Análises Exploratórias do Primeiro Conjunto de Dados**: Agregações por ano, mês, estado e tipo de degradação.
4.  **Carregamento e Pré-processamento do Segundo Conjunto de Dados (Desmatamento Anual - TerraBrasilis)**: Dados históricos de desmatamento anual.
5.  **Análises Exploratórias do Segundo Conjunto de Dados**: Tendências de longo prazo, ranking de estados e estatísticas.
6.  **Carregamento e Pré-processamento do Terceiro Conjunto de Dados (Histórico de Queimadas)**: Dados mensais de focos de queimadas.
7.  **Análises Exploratórias do Terceiro Conjunto de Dados**: Média mensal, mês de pico e crescimento anual.
8.  **Validação da Qualidade dos Dados**: Uso do Pandera para garantir a integridade dos dados.
9.  **Criação da Camada Bronze e Silver**: Enriquecimento e salvamento dos dados validados.

Time SQUAD A - Helena, Gabrielly, Carol
"""